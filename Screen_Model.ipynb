{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetwork(nn.Module):\n",
    "    def __init__(self, n_neurons_l1, n_neurons_l2):\n",
    "        super(ComplexNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, n_neurons_l1, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_neurons_l1, n_neurons_l2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_neurons_l2, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, n_neurons_l1):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, n_neurons_l1, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_neurons_l1, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, Xdata_folder, Ydata_folder, transform=None, device='cuda'):\n",
    "        self.Xdata_folder = Xdata_folder\n",
    "        self.Ydata_folder = Ydata_folder\n",
    "        self.transform = transform\n",
    "        self.image_list = os.listdir(Xdata_folder)\n",
    "        self.device = device\n",
    "        self.resize = transforms.Resize((300,512))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "        Ximage_path = os.path.join(self.Xdata_folder, image_name)\n",
    "        Yimage_path = os.path.join(self.Ydata_folder, image_name)\n",
    "        imageX = Image.open(Ximage_path)\n",
    "        imageY = Image.open(Yimage_path)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            imageX = self.transform(imageX)\n",
    "            imageY = self.transform(imageY)\n",
    "\n",
    "        # Move images to the desired device\n",
    "        imageX = imageX.to(self.device)\n",
    "        imageY = imageY.to(self.device)\n",
    "        # Resize images to the target size\n",
    "        imageX = self.resize(imageX)\n",
    "        imageY = self.resize(imageY)\n",
    "            \n",
    "\n",
    "        return imageX, imageY\n",
    "    \n",
    "folder_path = \"./Screen_data\"\n",
    "# Step 3: Load the data using DataLoader\n",
    "X_data_folder = os.path.join(folder_path, \"X_train\")\n",
    "Y_data_folder = os.path.join(folder_path, \"Y_train\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_transform = transforms.Compose([\n",
    "    #transforms.Resize((512, 300)),   # Resize the image to the desired size\n",
    "    transforms.ToTensor()            # Convert image to PyTorch tensor\n",
    "])\n",
    "\n",
    "batch_size = 16\n",
    "train_dataset = ImageDataset(X_data_folder, Y_data_folder, transform=data_transform, device=device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "Xv_data_folder = os.path.join(folder_path, \"X_val\")\n",
    "Yv_data_folder = os.path.join(folder_path, \"Y_val\")\n",
    "val_dataset = ImageDataset(Xv_data_folder, Yv_data_folder, transform=data_transform, device=device)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def get_vgg_features(images, vgg_model):\n",
    "    to_pil = transforms.ToPILImage()  # Transform to convert tensor to PIL Image\n",
    "    \n",
    "    image_list = []\n",
    "    for image in images:\n",
    "        image_pil = to_pil(image.cpu())  # Convert tensor to PIL Image\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        image_tensor = preprocess(image_pil).unsqueeze(0)\n",
    "        image_list.append(image_tensor)\n",
    "\n",
    "    images_preprocessed = torch.cat(image_list, dim=0).to(images.device)\n",
    "    \n",
    "    # Extract features from VGG model\n",
    "    features = vgg_model(images_preprocessed)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def perceptual_loss(input, target, vgg_model):\n",
    "    input_features = get_vgg_features(input, vgg_model)\n",
    "    target_features = get_vgg_features(target, vgg_model)\n",
    "    \n",
    "    loss = F.mse_loss(input_features, target_features)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def tv_loss(adv_patch):\n",
    "    # Calculate the total variation of the image\n",
    "    # The input image should be a PyTorch tensor of shape (C, H, W)\n",
    "    tv_h = torch.sum(torch.abs(adv_patch[:, :, 1:] - adv_patch[:, :, :-1]))\n",
    "    tv_w = torch.sum(torch.abs(adv_patch[:, 1:, :] - adv_patch[:, :-1, :]))\n",
    "    tv = (tv_h + tv_w) / torch.numel(adv_patch)\n",
    "    return tv\n",
    "\n",
    "\n",
    "def train(model, loss_fn, optimizer, train_loader, val_loader, n_epochs, p_loss_weight, tv_loss_weight, fine_tune=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    vgg_model = models.vgg19(pretrained=True).features[:9]  # Use a subset of VGG layers\n",
    "    \n",
    "    for param in vgg_model.parameters():\n",
    "        param.requires_grad = False  # Freeze VGG layers\n",
    "\n",
    "    if fine_tune:\n",
    "        # Freeze specific layers (Example: Freeze conv1 fine-tune conv2)\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in ['conv1.weight', 'conv1.bias']:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    vgg_model.to(device)\n",
    "    vgg_model.eval()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            \n",
    "            mse_loss = loss_fn(output, y[:, :3, ...])\n",
    "            p_loss = perceptual_loss(output, y[:, :3, ...], vgg_model)\n",
    "            tv_pred = tv_loss(output)\n",
    "            tv_y = tv_loss(y[:, :3, ...])\n",
    "            tv_diff_loss = torch.abs(tv_pred - tv_y)\n",
    "            \n",
    "            loss = mse_loss + p_loss_weight *p_loss + tv_loss_weight * tv_diff_loss\n",
    "            \"\"\"print('MSE: ',mse_loss)\n",
    "            print('PRC: ',p_loss)\n",
    "            print('TV: ',tv_diff_loss)\"\"\"\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        total_loss /= len(train_loader)\n",
    "        train_losses.append(total_loss)\n",
    "\n",
    "\n",
    "        # Calculate validation loss and append to list\n",
    "        val_loss = 0.0\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(val_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                output = model(x)\n",
    "            \n",
    "                mse_loss = loss_fn(output, y[:, :3, ...])\n",
    "                p_loss = perceptual_loss(output, y[:, :3, ...], vgg_model)\n",
    "                tv_pred = tv_loss(output)\n",
    "                tv_y = tv_loss(y[:, :3, ...])\n",
    "                tv_diff_loss = torch.abs(tv_pred - tv_y)\n",
    "                \n",
    "                v_loss = mse_loss + p_loss_weight *p_loss + tv_loss_weight * tv_diff_loss\n",
    "\n",
    "                val_loss += v_loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            \n",
    "\n",
    "        print(\"Epoch %d, Loss: %.4f\" % (epoch + 1, total_loss / len(train_loader)))\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_type = 'simple' #'complex' # \n",
    "n_epochs = 50\n",
    "if network_type == 'complex':\n",
    "    n_neurons_l1 = 128\n",
    "    n_neurons_l2 = 64\n",
    "    model = ComplexNetwork(n_neurons_l1, n_neurons_l2).to(device)\n",
    "elif network_type == 'simple':\n",
    "    n_neurons_l1 = 64\n",
    "    model = Network(n_neurons_l1).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "tv_loss_weight = 0.01 # Adjust this weight according to your preferences\n",
    "p_loss_weight = 0.02\n",
    "# Assuming a DataLoader named 'train_loader'\n",
    "train_losses, val_losses = train(model, loss_fn, optimizer, train_loader, val_loader, n_epochs, p_loss_weight, tv_loss_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs+1), train_losses, label='Train Loss', linewidth=3)\n",
    "plt.plot(range(1, n_epochs+1), val_losses, label='Validation Loss', linewidth=3)\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.ylabel('Loss', fontsize=20)\n",
    "#plt.xticks(range(1, n_epochs+1))\n",
    "#plt.title('Model Loss over Epochs', fontsize=14)\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "# Setting x and y limits\n",
    "plt.xlim(0, 51)\n",
    "plt.ylim(0, 0.08)\n",
    "plt.grid()\n",
    "# To save as a PDF (vectorized format)\n",
    "plt.savefig(os.path.join(folder_path, 'SIT-Net_train_Plot.pdf'), format='pdf', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_data_folder = os.path.join(folder_path, \"X_test\")\n",
    "Ytest_data_folder = os.path.join(folder_path, \"Y_test\")\n",
    "test_dataset = ImageDataset(Xtest_data_folder, Ytest_data_folder, transform=data_transform, device=device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "\n",
    "\n",
    "# Combine X and Y images using make_grid\n",
    "\n",
    "batch_X, batch_Y = next(iter(test_loader))\n",
    "brightness = torch.FloatTensor(batch_size, 1, 1, 1).uniform_(-0.1, 0.1).to('cuda')\n",
    "contrast = torch.cuda.FloatTensor(batch_size, 1, 1, 1).uniform_(0.8,1.2)\n",
    "noise = torch.cuda.FloatTensor(batch_X.size()).uniform_(-1, 1) * 0.1\n",
    "batch_R = model(batch_X)\n",
    "\n",
    "num_images = 10#batch_X.shape[0]\n",
    "\n",
    "plt.figure(figsize=(5*int(num_images), 11))\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "for i in range(num_images):\n",
    "    # Plot X image\n",
    "    plt.subplot(3, num_images, i + 1)\n",
    "    img_pil_X = ToPILImage()(batch_X[i])\n",
    "    plt.imshow(img_pil_X)\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"X Images\", fontsize=20, loc='left')\n",
    "    \n",
    "    # Plot Y image\n",
    "    plt.subplot(3, num_images, num_images + i + 1)\n",
    "    img_pil_Y = ToPILImage()(batch_Y[i])\n",
    "    plt.imshow(img_pil_Y)\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Y Images\", fontsize=20, loc='left')\n",
    "\n",
    "    # Plot res image\n",
    "    plt.subplot(3, num_images, 2*num_images + i + 1)\n",
    "    img_pil_R = ToPILImage()(batch_R[i])\n",
    "    plt.imshow(img_pil_R)\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Prediction\", fontsize=20, loc='left')\n",
    "plt.suptitle('Model Predictions vs Actual Data', fontsize=30, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(folder_path, 'Output.pdf'), format='pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model as a .pt file\n",
    "model_path = os.path.join(folder_path, f\"screen_{network_type}_model_SD_closet.pt\")\n",
    "torch.save(model, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
